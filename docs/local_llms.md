# Using Local LLMs With Prapti (Experimental)

## GPT4All

Prapti has experimental support for GPT4All, a CPU-based LLM inference engine.

You can use the Prapti configuration in the following markdown file as a starting point.

```
prapti/plugins/gpt4all_test_1.md
```

[Or just click here](../prapti/plugins/gpt4all_test_1.md)

The Python code that implements support for GPT4All in Prapti can be found at:

```
prapti/plugins/gpt4all_chat_responder.py
```

## Others

We plan to add support for other local LLM libraries. Watch this space, or contribute your own.
